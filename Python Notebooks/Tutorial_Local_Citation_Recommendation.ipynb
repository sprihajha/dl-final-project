{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6Ou4o3ywuzY"
   },
   "source": [
    "# Local-Citation-Recommendation\n",
    "Code for ECIR 2022 paper [Local Citation Recommendation with Hierarchical-Attention Text Encoder and SciBERT-based Reranking](https://link.springer.com/chapter/10.1007/978-3-030-99736-6_19)\n",
    "\n",
    "# Update 07-11-2022\n",
    "1. Cleaned the code for training and testing the prefetching system, to make it easier to read and to run.\n",
    "2. Simplify the information in config file, now there is only one global configuration file for prefetching and it is more readable.\n",
    "3. Optimize the GPU usage, now the system can be trained using a single GPU.\n",
    "4. Introduced the structure of the dataset and showed how to build your custom dataset and train a citation recommendation system on that.\n",
    "5. Provided a step-by-step tutorial on google colab, illustrating the whole process of training and testing of the entire prefetching and reranking system.\n",
    "\n",
    "# Hardware Requirement\n",
    "1. OS: Ubuntu 20.04 or 18.04\n",
    "2. 1 or more GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wppGf6Hjw_zM"
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AK2JER0TJrI3",
    "outputId": "255411e7-6211-4ed9-8778-abe75fb681b7"
   },
   "outputs": [],
   "source": [
    "!rm -r *\n",
    "!git clone https://github.com/nianlonggu/Local-Citation-Recommendation.git\n",
    "\n",
    "!pip install numpy tqdm matplotlib nltk transformers -q\n",
    "!pip install gdown -q\n",
    "!pip install cupy-cuda11x  ## suppose CUDA version >= 11.2\n",
    "import torch\n",
    "torch.__version__\n",
    "import nltk\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('stopwords',quiet=True)\n",
    "nltk.download('wordnet',quiet=True)\n",
    "import os\n",
    "os.chdir(\"Local-Citation-Recommendation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53L_-ZYpxRDx"
   },
   "source": [
    "# Download Glove Embedding \n",
    "For simplicity, we refer **MAIN** as the main folder of the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8jtK74cDhBw-",
    "outputId": "3d399df1-5a41-4918-ac79-a4248059da45"
   },
   "outputs": [],
   "source": [
    "!gdown  https://drive.google.com/uc?id=1T2R1H8UstSILH_JprUaPNY0fasxD2Txr; unzip model.zip; rm model.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlsPhzDMxp3F"
   },
   "source": [
    "# Prepare Dataset\n",
    "\n",
    "## Option 1: Build your custom dataset \n",
    "**This github repo contains a \"pseudo\" custom dataset that is actually ACL-200**\n",
    "\n",
    "The custom dataset will contain 5 components: contexts, papers, training/validation/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzA4jnytxrSp"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "contexts = json.load(open(\"data/custom/contexts.json\"))\n",
    "papers = json.load(open(\"data/custom/papers.json\"))\n",
    "train_set = json.load(open(\"data/custom/train.json\"))\n",
    "val_set = json.load(open(\"data/custom/val.json\"))\n",
    "test_set = json.load(open(\"data/custom/test.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcWkeEVqxuNf"
   },
   "source": [
    "### contexts contain the local contexts that cite a paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CdRkGqYx2Wb",
    "outputId": "eb25b0df-7768-44c8-dec0-f0794e20b277"
   },
   "outputs": [],
   "source": [
    "for key in contexts:\n",
    "    break\n",
    "contexts[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNNS3qSwxwc0"
   },
   "source": [
    "### papers contain the papers database, each paper has title and abstract.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6COKl-CUx7Nz",
    "outputId": "1a8e2422-75e1-415e-d05c-04c997cecd92"
   },
   "outputs": [],
   "source": [
    "for key in papers:\n",
    "    break\n",
    "papers[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etc8i422xzre"
   },
   "source": [
    "### train/val/test set contain the context_id (used for get the local context information and cited and citing papers information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rciarXCSx-Tx",
    "outputId": "ec399f50-7e4a-4dfb-9deb-a32eff6faa7c"
   },
   "outputs": [],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-94hgS2_yE66"
   },
   "source": [
    "positive_ids means the paper that is actually cited by the context. In this experiment the positive_ids always has one paper.\n",
    "\n",
    "You can create you own dataset with the same structure, and then train the citation recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjpeovp4yJHK"
   },
   "source": [
    "## Option 2: Download Processed Dataset\n",
    "You can also download our **processed dataset** (and **pretrained models**) from [Google Drive](https://drive.google.com/drive/folders/1QwQuJsBOGEESFTgl-7wWbqcig7vJNlQ2?usp=sharing)\n",
    "\n",
    "(There can be some additional information in the processed dataset other than what have been displayed in the examples above. They are irrelevant information.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2cEBGLHyYh1"
   },
   "source": [
    "# Prefetching Part\n",
    "In the following experiment, we use the \"custom\" dataset as an example. This dataset is the same as the ACL dataset. If you have created you dataset, you need to modify the config file at\n",
    "\n",
    "MAIN/src.prefetch/config/YOUR_DATASET_NAME/global_config.cfg\n",
    "## Training\n",
    "\n",
    "This can take around 2h for this custom dataset (the same as ACL-200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1SY5U-HVyI5r",
    "outputId": "8f4dd7be-bccc-4785-afee-18a4736cd1b9"
   },
   "outputs": [],
   "source": [
    "!cd src/prefetch; python run.py -config_file_path config/custom/global_config.cfg -mode train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIvdmqGI0T26"
   },
   "source": [
    "This code will automatically handle the loop of  training -> updating paper embeddings -> updating prefetched candidates for contructing triplets -> resuming training.\n",
    "\n",
    "\n",
    "In this case, the model checkpoint will be stored in the folder \"MAIN/model/prefetch/custom/\"; <br>\n",
    "              the paper embeddings are stored in \"MAIN/embedding/prefetch/custom/\"; <br>\n",
    "              the training log files are stored in \"MAIN/log/prefetch/custom/\"; \n",
    "              \n",
    "The file MAIN/log/prefetch/custom/validate_NN.log contains the validation performance of each checkpoint during training. With this information, we can pick up the best-performance model for testing. \n",
    "\n",
    "You can specify where to store the checkpoint, log files and other parammeters by modifying the config/custom/global_config.cfg configuration file.\n",
    "\n",
    "Note: **Before testing, after determining the best checkpoint, removing all the other checkpoints. If there are multiple checkpoints in MAIN/model/prefetch/custom/, the model will use the latest checkpoint by default.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_2XtXxM0a_d"
   },
   "source": [
    "## Testing\n",
    "To test the performance of the prefetching model, we need to first use the model checkpoint to compute the embedding for each paper in the database. This paper embedding is the index of the paper database, which is then used to perform nearest neighbor search. Then the next step is the test the prefetching performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY3bcxCj0iGJ"
   },
   "source": [
    "Here I download the pretrained ACL model only for demonstration. If you are training the model using your custom data, you can wait until the training ends and select the best checkpoint based on the validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZEs_ofWyyI7E",
    "outputId": "e4283c1b-3dfa-4cc2-8f83-4ea0e861ccc2"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.system(\"rm -r model/prefetch/custom\")\n",
    "    os.makedirs(\"model/prefetch/custom\")\n",
    "except:\n",
    "    pass\n",
    "!cd model/prefetch/custom; gdown  https://drive.google.com/uc?id=13J5mtRg6t3Lcsn6fCdLJ1pZhtXnpREEq;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kww9s0-Z2d7a"
   },
   "source": [
    "Step 1: Compute the embeddings of all papers in the database, using the trained text encoder (HAtten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bw2t6OSD2Ohy",
    "outputId": "a715d37e-9ee4-4319-aaef-b06404443145"
   },
   "outputs": [],
   "source": [
    "!cd src/prefetch; python run.py -config_file_path config/custom/global_config.cfg -mode compute_paper_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDfO1ruCuLEx"
   },
   "source": [
    "Step 2: Test the prefetching performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zGywipQo2D0m",
    "outputId": "a9f1e638-1953-4d51-e6b8-2fbb8f15882f"
   },
   "outputs": [],
   "source": [
    "!cd src/prefetch; python run.py -config_file_path config/custom/global_config.cfg -mode test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZOGpWivvHaJ"
   },
   "source": [
    "# Reranking Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lxb5LLyFvcSW"
   },
   "source": [
    "## Training\n",
    "In order to train the reranker, we need to first create the training dataset. More specifically, for each query in the training set, we first use the trained HAtten prefetcher to prefetch a list of (2000) candidates. Then within the 2000 prefetched candidates we can construct triplets to train the reranker.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgnfXYf5vioq"
   },
   "source": [
    "At this stage, we should have trained the prefetching model. We need to 1) compute paper embeddings for prefetching; 2) use the prefetching model to obtain prefetched candidates for each training example; 3) use the training examples with prefetched candidates to fine-tune SciBERT reranker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ewmk83WtvJ22",
    "outputId": "c89b7713-37ff-4e02-897c-db3571ea80dc"
   },
   "outputs": [],
   "source": [
    "!cd src/prefetch; python run.py -config_file_path config/custom/global_config.cfg -mode compute_paper_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26YhaS2nvJ4Y",
    "outputId": "6a05f01d-25b0-4b4a-8e5e-ca7241065107"
   },
   "outputs": [],
   "source": [
    "!cd src/prefetch; python run.py -config_file_path config/custom/global_config.cfg -mode get_training_examples_with_prefetched_ids_for_reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kgXC802ovJ6N",
    "outputId": "6b58083c-d1e9-42b8-e565-6bdc0260c883"
   },
   "outputs": [],
   "source": [
    "!cd src/prefetch; python run.py -config_file_path config/custom/global_config.cfg -mode get_val_examples_with_prefetched_ids_for_reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZBxCfc-2L-Q"
   },
   "source": [
    "After get the prefetched ids for training and validstion set, we can start training the reranker: (This can take 2.5 h to finish one epoch on this custom dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rZGfD8D92LfP",
    "outputId": "ed6286b5-0518-4ef8-c086-2da861054c52"
   },
   "outputs": [],
   "source": [
    "!cd src/rerank; python train.py -config_file_path  config/custom/scibert/training_NN_prefetch.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HFXRm5p3tC2"
   },
   "source": [
    "# Use The HAtten Prefetcher and the SciBERT Reranker in Python Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsDs4C94339-"
   },
   "source": [
    "Before run this code, we should have trained the prefetching and the reranking models and know the path to the checkpoint of the saved model. (Here I download the pretrained model on ACL-200 for demonstration. If you train using your custom data, skip this and put the trained models' checkpoint to the corresponding model folder.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44o7PBnP2Lho",
    "outputId": "a149a542-e2a9-49fd-928d-6e6ba2973d3f"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.system(\"rm -r model/prefetch/custom\")\n",
    "    os.makedirs(\"model/prefetch/custom\")\n",
    "except:\n",
    "    pass\n",
    "!cd model/prefetch/custom; gdown  https://drive.google.com/uc?id=13J5mtRg6t3Lcsn6fCdLJ1pZhtXnpREEq;\n",
    "\n",
    "try:\n",
    "    os.system(\"rm -r model/rerank/custom\")\n",
    "    os.makedirs(\"model/rerank/custom/scibert/NN_prefetch\")\n",
    "except:\n",
    "    pass\n",
    "!cd model/rerank/custom/scibert/NN_prefetch; gdown  https://drive.google.com/uc?id=1DmSw6HR2W4fbUKp24K1_TREPhlLiQuEb;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTpuaoyf7HQ1"
   },
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x56aHUSJ7KlA"
   },
   "outputs": [],
   "source": [
    "from citation_recommender import * \n",
    "prefetcher = Prefetcher( \n",
    "       model_path=\"model/prefetch/custom/model_batch_35000.pt\",\n",
    "       embedding_path=\"embedding/prefetch/custom/paper_embedding.pkl\", ## make sure the papers embeddings have been computed\n",
    "       gpu_list= [0,] \n",
    ")\n",
    "reranker = Reranker( model_path = \"model/rerank/custom/scibert/NN_prefetch/model_batch_91170.pt\", \n",
    "                     gpu_list = [0,] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-mTXfeA7be4"
   },
   "source": [
    "## Get paper recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vj357UWI6cNg"
   },
   "source": [
    "Then we can construct a query, and use the query to find n most relevant papers. The query is a dictionary containing 3 keys: \"citing_title\",\"citing_abstract\" and \"local_context\". We can get some real example from the test set, or we can contruct a simple query as follows:\n",
    "\n",
    "(## You can specify any other values, e.g., 100, 1000 or 2000. Note that the reranking time is proportional to the number of candidates to rerank.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTrU2bbF87-B"
   },
   "outputs": [],
   "source": [
    "idx = 100\n",
    "context_info = contexts[test_set[idx][\"context_id\"]]\n",
    "citing_id = context_info[\"citing_id\"]\n",
    "refid = context_info[\"refid\"]  ## The ground-truth cited paper\n",
    "\n",
    "local_context = context_info[\"masked_text\"]\n",
    "citing_paper = papers[citing_id]\n",
    "citing_title = citing_paper[\"title\"]\n",
    "citing_abstract = citing_paper[\"abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HxHPm-bC9WqL",
    "outputId": "1fcc55a9-1c19-4bf8-e968-85b45548e697"
   },
   "outputs": [],
   "source": [
    "citing_title, citing_abstract, local_context, refid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yEtPzDpl556o",
    "outputId": "76c40c6b-40fc-4e68-86f0-49e4c5df83f9"
   },
   "outputs": [],
   "source": [
    "candi_list = prefetcher.get_top_n(\n",
    "  {\n",
    "      \"citing_title\":citing_title,\n",
    "      \"citing_abstract\":citing_abstract,\n",
    "      \"local_context\":local_context\n",
    "  }, 500\n",
    ")\n",
    "print(candi_list[:10])\n",
    "\n",
    "for pos, cadi_id in enumerate(candi_list):\n",
    "    if cadi_id == refid:\n",
    "        print(\"The truely cited paper's id %s appears in position: %d among the prefetched ids.\"%( refid, pos ))\n",
    "        break\n",
    "if refid not in candi_list:\n",
    "    print(\"The truely cited paper's id %s is not included in the prefetched ids\"%( refid ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9RG8bdw7qEZ"
   },
   "source": [
    "Then we can rerank the prefetched candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sv0_ECwG7ggf"
   },
   "outputs": [],
   "source": [
    "candidate_list = [  {\"paper_id\": pid,\n",
    "                     \"title\":papers[pid].get(\"title\",\"\"),\n",
    "                     \"abstract\":papers[pid].get(\"abstract\",\"\")}\n",
    "                            for pid in candi_list ] \n",
    "# start reranking\n",
    "reranked_candidate_list = reranker.rerank( citing_title,citing_abstract,local_context, candidate_list )\n",
    "reranked_candidate_ids = [item[\"paper_id\"] for item in reranked_candidate_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBsofFuA7giN"
   },
   "outputs": [],
   "source": [
    "for pos, cadi_id in enumerate(reranked_candidate_ids):\n",
    "    if cadi_id == refid:\n",
    "        print(\"The truely cited paper's id %s appears in position: %d among the reranked ids.\"%( refid, pos ))\n",
    "        break\n",
    "if refid not in reranked_candidate_ids:\n",
    "    print(\"The truely cited paper's id %s is not included in the reranked ids\"%( refid ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkn0tcWVBMEj"
   },
   "source": [
    "## Evaluation of the whole prefetching and reranking pipeline\n",
    "\n",
    "\n",
    "We use HAtten to prefetch 100 candidates and then rerank them and we record the Recall@10 in the final recommendations  (We test this on 100 test examples only for demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qxfqf47n7gka",
    "outputId": "c69c73ad-21dd-4f6a-f6e4-0c8d12eaadd1"
   },
   "outputs": [],
   "source": [
    "hit_list = []\n",
    "top_K = 10\n",
    "\n",
    "for idx in tqdm(range(100)):\n",
    "\n",
    "    context_info = contexts[test_set[idx][\"context_id\"]]\n",
    "    citing_id = context_info[\"citing_id\"]\n",
    "    refid = context_info[\"refid\"]  ## The ground-truth cited paper\n",
    "\n",
    "    local_context = context_info[\"masked_text\"]\n",
    "    citing_paper = papers[citing_id]\n",
    "    citing_title = citing_paper[\"title\"]\n",
    "    citing_abstract = citing_paper[\"abstract\"]\n",
    "\n",
    "    candi_list = prefetcher.get_top_n(\n",
    "        {\n",
    "            \"citing_title\":citing_title,\n",
    "            \"citing_abstract\":citing_abstract,\n",
    "            \"local_context\":local_context\n",
    "        }, 100  ## 100 candidates \n",
    "    )\n",
    "\n",
    "    candidate_list = [  {\"paper_id\": pid,\n",
    "                     \"title\":papers[pid].get(\"title\",\"\"),\n",
    "                     \"abstract\":papers[pid].get(\"abstract\",\"\")}\n",
    "                            for pid in candi_list ] \n",
    "    # start reranking\n",
    "    reranked_candidate_list = reranker.rerank( citing_title,citing_abstract,local_context, candidate_list )\n",
    "    reranked_candidate_ids = [item[\"paper_id\"] for item in reranked_candidate_list]\n",
    "\n",
    "    hit_list.append( refid in reranked_candidate_ids[:top_K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbgBOFnLD0Jm",
    "outputId": "e47cb371-d743-4d97-8c0b-1c5633a8d789"
   },
   "outputs": [],
   "source": [
    "print(\"The average recall@%d: %.4f\"%( top_K, np.mean(hit_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_gbho5MCwlM"
   },
   "source": [
    "This value is close to the results on ACL-200 in Table 4 in the paper, where we tested using full test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isXIJvhUDL7e"
   },
   "source": [
    "# References\n",
    "When using our code or models for your application, please cite the following paper:\n",
    "\n",
    "```\n",
    "@InProceedings{10.1007/978-3-030-99736-6_19,\n",
    "author=\"Gu, Nianlong\n",
    "and Gao, Yingqiang\n",
    "and Hahnloser, Richard H. R.\",\n",
    "editor=\"Hagen, Matthias\n",
    "and Verberne, Suzan\n",
    "and Macdonald, Craig\n",
    "and Seifert, Christin\n",
    "and Balog, Krisztian\n",
    "and N{\\o}rv{\\aa}g, Kjetil\n",
    "and Setty, Vinay\",\n",
    "title=\"Local Citation Recommendation with Hierarchical-Attention Text Encoder and SciBERT-Based Reranking\",\n",
    "booktitle=\"Advances in Information Retrieval\",\n",
    "year=\"2022\",\n",
    "publisher=\"Springer International Publishing\",\n",
    "address=\"Cham\",\n",
    "pages=\"274--288\",\n",
    "abstract=\"The goal of local citation recommendation is to recommend a missing reference from the local citation context and optionally also from the global context. To balance the tradeoff between speed and accuracy of citation recommendation in the context of a large-scale paper database, a viable approach is to first prefetch a limited number of relevant documents using efficient ranking methods and then to perform a fine-grained reranking using more sophisticated models. In that vein, BM25 has been found to be a tough-to-beat approach to prefetching, which is why recent work has focused mainly on the reranking step. Even so, we explore prefetching with nearest neighbor search among text embeddings constructed by a hierarchical attention network. When coupled with a SciBERT reranker fine-tuned on local citation recommendation tasks, our hierarchical Attention encoder (HAtten) achieves high prefetch recall for a given number of candidates to be reranked. Consequently, our reranker requires fewer prefetch candidates to rerank, yet still achieves state-of-the-art performance on various local citation recommendation datasets such as ACL-200, FullTextPeerRead, RefSeer, and arXiv.\",\n",
    "isbn=\"978-3-030-99736-6\"\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "e6Ou4o3ywuzY",
    "fjpeovp4yJHK"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
